README

## 这个仓库用来存储我的论文

### 1.网络压缩相关论文

（1）Efficient Processing of Deep Neural Networks: A Tutorial and Survey 

- **蒸馏论文**

  1. 17-Training Shallow and Thin Networks for Acceleration via Knowledge distillation with conditional adversarial Nerworks
  2. 19-structured knowledge distillation for semantic segmentation
  3. 2015Distilling the Knowledge in a Neural Network
  4. 2017-learning-efficient-object-detection-models-with-knowledge-distillation
  5. 2017 like what you like:knowledge Distill via Neuron selectivity transfer
  6. 2017-Li_Mimicking_Very_Efficient_network_for_object_detection_CVPR_2017_paper
  7. 2017 Model Distillation with Knowledge Transffer from Face Classification to Alignment and Verification
  8. 2017Paying more attention to attention:improving the performance of convolutional neural networks via attention transfer
  9. 2018 Interpreting Deep Classifiers by Visual Distillation of Dark Knowledge
  10. 2019 layer-level knowledge distillation for deep neural network learning
  11. 2019Relational knowledge distillation
  12. DarkRank: Accelerating Deep Metric Learning via cross Sample Similarities
  13. Extending detection with privileged information via generalized distillation
  14. FitNets：hints for thin deep nets.pdf
  15. graph-based knowledge distillation by multi-head attention network
  16. Learning Efficient Detector with Semi-supervised Adaptive Distillation
  17. Quantization Mimic: Towards very tiny CNN for object detection
  18. Rocket Lanunching: A universal and efficient frame work for trainging well-performing light net
  19. Wang_Distilling_Object_Detectors_With_Fine-Grained_Feature_Imitation_CVPR_2019_paper

- **人脸识别相关蒸馏**

  1. 2016 Face Model Compression by Distilling Knowledge from Neurons
  2. 2017基于深度特征蒸馏的人脸识别
  3. 2018Model distillation with knowledge transfer from face classification to alignment and verification
  4. 2019 Deep Face Recognition Model Compression via Knowledge Transfer and Distillation
  5. 2019Triplet Distillation for Deep Face Recognition
  6. low-resolution face recognition in the wild via selective knowledge distillation 
  7. Wang_Improved_Knowledge_Distillation_for_Training_Fast_Low_Resolution_Face_Recognition_ICCVW_2019_paper

- 低秩分解论文

  1. TensorReview
  2. Tensor Decompositions and Applications
  3. speeding up convolutional neural networks with low rank expansions
  4. Speeding-up_Convolutional_Neural_Networks_Using_Fi
  5. Rigamonti_Learning_Separable_Filters_2013_CVPR_paper
  6. Learning low-rank approximation for CNNs
  7. High performance ultra-low-precision convolutions on mobile devices
  8. Higher Order Orthogonal Iteration of Tensors(HOOI) and its Relation to PCA and GLRAM
  9. convolutinal neural networks with low-rank regulatization
  10. Compression of deep convolutional neural networks for fast and low power mobile applications
  11. Accelerating Very Deep Convolutional Networks for Classification and Detection

- 网络剪枝论文

  1. THE LOTTERY TICKET HYPOTHESIS:FINDING SPARSE, TRAINABLE NEURAL NETWORKS

  2. 2015_accelerating very deep convolutional networks for classification and detection

  3. 2019_cvpr_filter pruning via geometric median for deep convolutional neural networks acceleration

     

#### 2.NAS相关论文

（1）MnasNet: Platform-Aware Neural Architecture Search for Mobile

（2）DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH

（3）NAS-BENCH-201: EXTENDING THE SCOPE OF REPRODUCIBLE NEURAL ARCHITECTURE SEARCH

#### 3.few shot learning

1. 2020.3 Generalizing from a Few Examples: A Survey on Few-Shot Learning